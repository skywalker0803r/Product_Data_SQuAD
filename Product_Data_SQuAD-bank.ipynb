{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from torch.optim.optimizer import Optimizer\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "from pytorch_lightning import seed_everything\n",
    "import os\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    }
   ],
   "source": [
    "def set_seed(seed = int):\n",
    "    '''Sets the seed of the entire notebook so results are the same every time we run.\n",
    "    This is for REPRODUCIBILITY.'''\n",
    "    np.random.seed(seed)\n",
    "    random_state = np.random.RandomState(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    seed_everything(seed)\n",
    "    return random_state\n",
    "random_state = set_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MADGRAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import TYPE_CHECKING, Any, Callable, Optional\n",
    "\n",
    "if TYPE_CHECKING:\n",
    "    from torch.optim.optimizer import _params_t\n",
    "else:\n",
    "    _params_t = Any\n",
    "\n",
    "class MADGRAD(Optimizer):\n",
    "\n",
    "    def __init__(\n",
    "        self, params: _params_t, lr: float = 1e-2, momentum: float = 0.9, weight_decay: float = 0, eps: float = 1e-6,\n",
    "    ):\n",
    "        if momentum < 0 or momentum >= 1:\n",
    "            raise ValueError(f\"Momentum {momentum} must be in the range [0,1]\")\n",
    "        if lr <= 0:\n",
    "            raise ValueError(f\"Learning rate {lr} must be positive\")\n",
    "        if weight_decay < 0:\n",
    "            raise ValueError(f\"Weight decay {weight_decay} must be non-negative\")\n",
    "        if eps < 0:\n",
    "            raise ValueError(f\"Eps must be non-negative\")\n",
    "\n",
    "        defaults = dict(lr=lr, eps=eps, momentum=momentum, weight_decay=weight_decay)\n",
    "        super().__init__(params, defaults)\n",
    "\n",
    "    @property\n",
    "    def supports_memory_efficient_fp16(self) -> bool:\n",
    "        return False\n",
    "\n",
    "    @property\n",
    "    def supports_flat_params(self) -> bool:\n",
    "        return True\n",
    "\n",
    "    def step(self, closure: Optional[Callable[[], float]] = None) -> Optional[float]:\n",
    "\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "\n",
    "        if 'k' not in self.state:\n",
    "            self.state['k'] = torch.tensor([0], dtype=torch.long)\n",
    "        k = self.state['k'].item()\n",
    "\n",
    "        for group in self.param_groups:\n",
    "            eps = group[\"eps\"]\n",
    "            lr = group[\"lr\"] + eps\n",
    "            decay = group[\"weight_decay\"]\n",
    "            momentum = group[\"momentum\"]\n",
    "\n",
    "            ck = 1 - momentum\n",
    "            lamb = lr * math.pow(k + 1, 0.5)\n",
    "\n",
    "            for p in group[\"params\"]:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                grad = p.grad.data\n",
    "                state = self.state[p]\n",
    "\n",
    "                if \"grad_sum_sq\" not in state:\n",
    "                    state[\"grad_sum_sq\"] = torch.zeros_like(p.data).detach()\n",
    "                    state[\"s\"] = torch.zeros_like(p.data).detach()\n",
    "                    if momentum != 0:\n",
    "                        state[\"x0\"] = torch.clone(p.data).detach()\n",
    "\n",
    "                if momentum != 0.0 and grad.is_sparse:\n",
    "                    raise RuntimeError(\"momentum != 0 is not compatible with sparse gradients\")\n",
    "\n",
    "                grad_sum_sq = state[\"grad_sum_sq\"]\n",
    "                s = state[\"s\"]\n",
    "\n",
    "                # Apply weight decay\n",
    "                if decay != 0:\n",
    "                    if grad.is_sparse:\n",
    "                        raise RuntimeError(\"weight_decay option is not compatible with sparse gradients\")\n",
    "\n",
    "                    grad.add_(p.data, alpha=decay)\n",
    "\n",
    "                if grad.is_sparse:\n",
    "                    grad = grad.coalesce()\n",
    "                    grad_val = grad._values()\n",
    "\n",
    "                    p_masked = p.sparse_mask(grad)\n",
    "                    grad_sum_sq_masked = grad_sum_sq.sparse_mask(grad)\n",
    "                    s_masked = s.sparse_mask(grad)\n",
    "\n",
    "                    # Compute x_0 from other known quantities\n",
    "                    rms_masked_vals = grad_sum_sq_masked._values().pow(1 / 3).add_(eps)\n",
    "                    x0_masked_vals = p_masked._values().addcdiv(s_masked._values(), rms_masked_vals, value=1)\n",
    "\n",
    "                    # Dense + sparse op\n",
    "                    grad_sq = grad * grad\n",
    "                    grad_sum_sq.add_(grad_sq, alpha=lamb)\n",
    "                    grad_sum_sq_masked.add_(grad_sq, alpha=lamb)\n",
    "\n",
    "                    rms_masked_vals = grad_sum_sq_masked._values().pow_(1 / 3).add_(eps)\n",
    "\n",
    "                    s.add_(grad, alpha=lamb)\n",
    "                    s_masked._values().add_(grad_val, alpha=lamb)\n",
    "\n",
    "                    # update masked copy of p\n",
    "                    p_kp1_masked_vals = x0_masked_vals.addcdiv(s_masked._values(), rms_masked_vals, value=-1)\n",
    "                    # Copy updated masked p to dense p using an add operation\n",
    "                    p_masked._values().add_(p_kp1_masked_vals, alpha=-1)\n",
    "                    p.data.add_(p_masked, alpha=-1)\n",
    "                else:\n",
    "                    if momentum == 0:\n",
    "                        # Compute x_0 from other known quantities\n",
    "                        rms = grad_sum_sq.pow(1 / 3).add_(eps)\n",
    "                        x0 = p.data.addcdiv(s, rms, value=1)\n",
    "                    else:\n",
    "                        x0 = state[\"x0\"]\n",
    "\n",
    "                    # Accumulate second moments\n",
    "                    grad_sum_sq.addcmul_(grad, grad, value=lamb)\n",
    "                    rms = grad_sum_sq.pow(1 / 3).add_(eps)\n",
    "\n",
    "                    # Update s\n",
    "                    s.data.add_(grad, alpha=lamb)\n",
    "\n",
    "                    # Step\n",
    "                    if momentum == 0:\n",
    "                        p.data.copy_(x0.addcdiv(s, rms, value=-1))\n",
    "                    else:\n",
    "                        z = x0.addcdiv(s, rms, value=-1)\n",
    "\n",
    "                        # p is a moving average of z\n",
    "                        p.data.mul_(1 - ck).add_(z, alpha=ck)\n",
    "\n",
    "\n",
    "        self.state['k'] += 1\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>46A</th>\n",
       "      <th>47A</th>\n",
       "      <th>78</th>\n",
       "      <th>LCBK</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 SIGNED COMMERCIAL INVOICE IN 3 ORIGINALS AND...</td>\n",
       "      <td>1 ALL DOCUMENTS MUST BE IN ENGLISH2 ALL DOCUME...</td>\n",
       "      <td>UPON RECEIPT OF CREDIT COMPLIANT DOCUMENTS AT ...</td>\n",
       "      <td>ICICI BANK LTDSHALIMAR TOWER 3154 MGMARGHAZRAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 SIGNED COMMERCIAL INVOICE IN ORIGINAL AND 4 ...</td>\n",
       "      <td>A ALL DOCUMENTS AND DRAFTS IF CALLED FOR UNDER...</td>\n",
       "      <td>IN REIMBURSEMENT OF NEGOTIATION MADE BY YOUIN ...</td>\n",
       "      <td>NATIONAL COMMERCIAL BANK THEHEAD OFFICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1SIGNED COMMERCIAL INVOICE IN 03 ORIGINALS AND...</td>\n",
       "      <td>ALL DOCUMENTS MUST MADE IN ENGLISHALL DOCUMENT...</td>\n",
       "      <td>UPON RECEIPT OF ALL DOCUMENTS SENT TO USVIETNA...</td>\n",
       "      <td>VIET NAM BANK FOR AGRICULTURE AND RURAL DEVELO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SIGNED COMMERCIAL INVOICE IN TRIPLICATE INDICA...</td>\n",
       "      <td>ALL DRAFTS DRAWN HEREUNDER MUST BE MARKED DRAW...</td>\n",
       "      <td>ALL DOCUMENTS INCLUDING BENEFICIARYS DRAFTS MU...</td>\n",
       "      <td>THE MIZUHO BANKLTDHEAD OFFICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SIGNED COMMERCIAL INVOICE IN TRIPLICATE INDICA...</td>\n",
       "      <td>ALL DRAFTS DRAWN HEREUNDER MUST BE MARKED DRAW...</td>\n",
       "      <td>ALL DOCUMENTS INCLUDING BENEFICIARYS DRAFTS MU...</td>\n",
       "      <td>THE MIZUHO BANKLTDHEAD OFFICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8403</th>\n",
       "      <td>1SIGNED COMMERCIAL INVOICE IN 3 ORIGINALS INDI...</td>\n",
       "      <td>1 ALL DOCUMENTS MUST BE DATED SIGNED AND ISSUE...</td>\n",
       "      <td>THE AMOUNT OF EACH DRAWING UNDER THIS CREDIT M...</td>\n",
       "      <td>AGRICULTURAL BANK OF CHINA LTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8413</th>\n",
       "      <td>COMMERCIAL INVOICE IN 3 ORIGINALx000D 23 SET O...</td>\n",
       "      <td>TTREIMBURSEMENT  PROHIBITEDx000D INSURANCE IS ...</td>\n",
       "      <td>ALL DOCS TO BE SENT DIRECTLY TO US 3-10-19 MIN...</td>\n",
       "      <td>SUMITOMO MITSUI BANKING CORPORATION JAPAN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8414</th>\n",
       "      <td>SIGNED AND STAMPED COMMERCIAL INVOICE IN 3 ORI...</td>\n",
       "      <td>KINDLY ASSIST US TO RELAY THIS CREDIT TO CITIT...</td>\n",
       "      <td>TO NEGOTIATING BANK ONLY  PLEASE FORWARD ALL D...</td>\n",
       "      <td>CTBC BANK COLTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8415</th>\n",
       "      <td>SIGNED AND STAMPED COMMERCIAL INVOICE IN 3 ORI...</td>\n",
       "      <td>KINDLY ASSIST US TO RELAY THIS CREDIT TO CITIT...</td>\n",
       "      <td>TO NEGOTIATING BANK ONLY  PLEASE FORWARD ALL D...</td>\n",
       "      <td>CTBC BANK COLTD</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8416</th>\n",
       "      <td>SIGNED AND STAMPED COMMERCIAL INVOICE IN 3 ORI...</td>\n",
       "      <td>KINDLY ASSIST US TO RELAY THIS CREDIT TO CITIT...</td>\n",
       "      <td>TO NEGOTIATING BANK ONLY  PLEASE FORWARD ALL D...</td>\n",
       "      <td>CTBC BANK COLTD</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3643 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    46A  \\\n",
       "3     1 SIGNED COMMERCIAL INVOICE IN 3 ORIGINALS AND...   \n",
       "4     1 SIGNED COMMERCIAL INVOICE IN ORIGINAL AND 4 ...   \n",
       "5     1SIGNED COMMERCIAL INVOICE IN 03 ORIGINALS AND...   \n",
       "7     SIGNED COMMERCIAL INVOICE IN TRIPLICATE INDICA...   \n",
       "9     SIGNED COMMERCIAL INVOICE IN TRIPLICATE INDICA...   \n",
       "...                                                 ...   \n",
       "8403  1SIGNED COMMERCIAL INVOICE IN 3 ORIGINALS INDI...   \n",
       "8413  COMMERCIAL INVOICE IN 3 ORIGINALx000D 23 SET O...   \n",
       "8414  SIGNED AND STAMPED COMMERCIAL INVOICE IN 3 ORI...   \n",
       "8415  SIGNED AND STAMPED COMMERCIAL INVOICE IN 3 ORI...   \n",
       "8416  SIGNED AND STAMPED COMMERCIAL INVOICE IN 3 ORI...   \n",
       "\n",
       "                                                    47A  \\\n",
       "3     1 ALL DOCUMENTS MUST BE IN ENGLISH2 ALL DOCUME...   \n",
       "4     A ALL DOCUMENTS AND DRAFTS IF CALLED FOR UNDER...   \n",
       "5     ALL DOCUMENTS MUST MADE IN ENGLISHALL DOCUMENT...   \n",
       "7     ALL DRAFTS DRAWN HEREUNDER MUST BE MARKED DRAW...   \n",
       "9     ALL DRAFTS DRAWN HEREUNDER MUST BE MARKED DRAW...   \n",
       "...                                                 ...   \n",
       "8403  1 ALL DOCUMENTS MUST BE DATED SIGNED AND ISSUE...   \n",
       "8413  TTREIMBURSEMENT  PROHIBITEDx000D INSURANCE IS ...   \n",
       "8414  KINDLY ASSIST US TO RELAY THIS CREDIT TO CITIT...   \n",
       "8415  KINDLY ASSIST US TO RELAY THIS CREDIT TO CITIT...   \n",
       "8416  KINDLY ASSIST US TO RELAY THIS CREDIT TO CITIT...   \n",
       "\n",
       "                                                     78  \\\n",
       "3     UPON RECEIPT OF CREDIT COMPLIANT DOCUMENTS AT ...   \n",
       "4     IN REIMBURSEMENT OF NEGOTIATION MADE BY YOUIN ...   \n",
       "5     UPON RECEIPT OF ALL DOCUMENTS SENT TO USVIETNA...   \n",
       "7     ALL DOCUMENTS INCLUDING BENEFICIARYS DRAFTS MU...   \n",
       "9     ALL DOCUMENTS INCLUDING BENEFICIARYS DRAFTS MU...   \n",
       "...                                                 ...   \n",
       "8403  THE AMOUNT OF EACH DRAWING UNDER THIS CREDIT M...   \n",
       "8413  ALL DOCS TO BE SENT DIRECTLY TO US 3-10-19 MIN...   \n",
       "8414  TO NEGOTIATING BANK ONLY  PLEASE FORWARD ALL D...   \n",
       "8415  TO NEGOTIATING BANK ONLY  PLEASE FORWARD ALL D...   \n",
       "8416  TO NEGOTIATING BANK ONLY  PLEASE FORWARD ALL D...   \n",
       "\n",
       "                                                   LCBK  \n",
       "3     ICICI BANK LTDSHALIMAR TOWER 3154 MGMARGHAZRAT...  \n",
       "4               NATIONAL COMMERCIAL BANK THEHEAD OFFICE  \n",
       "5     VIET NAM BANK FOR AGRICULTURE AND RURAL DEVELO...  \n",
       "7                         THE MIZUHO BANKLTDHEAD OFFICE  \n",
       "9                         THE MIZUHO BANKLTDHEAD OFFICE  \n",
       "...                                                 ...  \n",
       "8403                     AGRICULTURAL BANK OF CHINA LTD  \n",
       "8413          SUMITOMO MITSUI BANKING CORPORATION JAPAN  \n",
       "8414                                    CTBC BANK COLTD  \n",
       "8415                                    CTBC BANK COLTD  \n",
       "8416                                    CTBC BANK COLTD  \n",
       "\n",
       "[3643 rows x 4 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('46A47A78LCBK.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string_X_train</th>\n",
       "      <th>Y_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 SIGNED COMMERCIAL INVOICE IN 3 ORIGINALS AND...</td>\n",
       "      <td>ICICI BANK LTDSHALIMAR TOWER 3154 MGMARGHAZRAT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 SIGNED COMMERCIAL INVOICE IN ORIGINAL AND 4 ...</td>\n",
       "      <td>NATIONAL COMMERCIAL BANK THEHEAD OFFICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1SIGNED COMMERCIAL INVOICE IN 03 ORIGINALS AND...</td>\n",
       "      <td>VIET NAM BANK FOR AGRICULTURE AND RURAL DEVELO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>SIGNED COMMERCIAL INVOICE IN TRIPLICATE INDICA...</td>\n",
       "      <td>THE MIZUHO BANKLTDHEAD OFFICE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SIGNED COMMERCIAL INVOICE IN TRIPLICATE INDICA...</td>\n",
       "      <td>THE MIZUHO BANKLTDHEAD OFFICE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      string_X_train  \\\n",
       "3  1 SIGNED COMMERCIAL INVOICE IN 3 ORIGINALS AND...   \n",
       "4  1 SIGNED COMMERCIAL INVOICE IN ORIGINAL AND 4 ...   \n",
       "5  1SIGNED COMMERCIAL INVOICE IN 03 ORIGINALS AND...   \n",
       "7  SIGNED COMMERCIAL INVOICE IN TRIPLICATE INDICA...   \n",
       "9  SIGNED COMMERCIAL INVOICE IN TRIPLICATE INDICA...   \n",
       "\n",
       "                                             Y_label  \n",
       "3  ICICI BANK LTDSHALIMAR TOWER 3154 MGMARGHAZRAT...  \n",
       "4            NATIONAL COMMERCIAL BANK THEHEAD OFFICE  \n",
       "5  VIET NAM BANK FOR AGRICULTURE AND RURAL DEVELO...  \n",
       "7                      THE MIZUHO BANKLTDHEAD OFFICE  \n",
       "9                      THE MIZUHO BANKLTDHEAD OFFICE  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "df['tab'] = ' '\n",
    "df['string_X_train'] = df['46A']+df['tab']+df['47A']+df['tab']+df['78']\n",
    "df = df[['string_X_train','LCBK']]\n",
    "df = df.rename(columns={'LCBK':'Y_label'})\n",
    "df['string_X_train'] = [str(i) for i in df['string_X_train']]\n",
    "df['Y_label'] = [str(i) for i in df['Y_label']]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3643, 2)\n",
      "(1415, 2)\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['string_X_train','Y_label'],axis=0).reset_index(drop=True)\n",
    "print(df.shape)\n",
    "keep_lst = []\n",
    "for i in range(len(df)):\n",
    "    if df.iloc[i]['Y_label'] in df.iloc[i]['string_X_train']:\n",
    "        keep_lst.append(i)\n",
    "df = df.iloc[keep_lst]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>string_X_train</th>\n",
       "      <th>Y_label</th>\n",
       "      <th>string_Y_1</th>\n",
       "      <th>string_Y_2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>SIGNED COMMERCIAL INVOICE IN 3 ORIGINALS INDIC...</td>\n",
       "      <td>CTBC BANK CO LTD</td>\n",
       "      <td>1860</td>\n",
       "      <td>1876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>SIGNED COMMERCIAL INVOICE IN 2 ORIGINALINDICAT...</td>\n",
       "      <td>CTBC BANK CO LTD</td>\n",
       "      <td>4924</td>\n",
       "      <td>4940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>SIGNED COMMERCIAL INVOICE IN 2 ORIGINALINDICAT...</td>\n",
       "      <td>CTBC BANK CO LTD</td>\n",
       "      <td>4924</td>\n",
       "      <td>4940</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>1SIGNED COMMERCIAL INVOICE IN DUPLICATE SHOWIN...</td>\n",
       "      <td>CHANG HWA COMMERCIAL BANK LTD</td>\n",
       "      <td>5345</td>\n",
       "      <td>5374</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1FULL SET OF ORIGINALS AND 2 NON-NEGOTIABLE CO...</td>\n",
       "      <td>STANDARD CHARTERED BANK</td>\n",
       "      <td>1596</td>\n",
       "      <td>1619</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       string_X_train  \\\n",
       "8   SIGNED COMMERCIAL INVOICE IN 3 ORIGINALS INDIC...   \n",
       "9   SIGNED COMMERCIAL INVOICE IN 2 ORIGINALINDICAT...   \n",
       "10  SIGNED COMMERCIAL INVOICE IN 2 ORIGINALINDICAT...   \n",
       "13  1SIGNED COMMERCIAL INVOICE IN DUPLICATE SHOWIN...   \n",
       "19  1FULL SET OF ORIGINALS AND 2 NON-NEGOTIABLE CO...   \n",
       "\n",
       "                          Y_label  string_Y_1  string_Y_2  \n",
       "8                CTBC BANK CO LTD        1860        1876  \n",
       "9                CTBC BANK CO LTD        4924        4940  \n",
       "10               CTBC BANK CO LTD        4924        4940  \n",
       "13  CHANG HWA COMMERCIAL BANK LTD        5345        5374  \n",
       "19        STANDARD CHARTERED BANK        1596        1619  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def str2index(context,string):\n",
    "    ys = context.find(string)\n",
    "    ye = ys + len(string)\n",
    "    return ys,ye\n",
    "\n",
    "ys_lst = []\n",
    "ye_lst = []\n",
    "for i in range(len(df)):\n",
    "    context= df['string_X_train'].values[i]\n",
    "    string = df['Y_label'].values[i]\n",
    "    ys,ye = str2index(context,string)\n",
    "    ys_lst.append(ys)\n",
    "    ye_lst.append(ye)\n",
    "df['string_Y_1'] = ys_lst\n",
    "df['string_Y_2'] = ye_lst\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CTBC BANK CO LTD CTBC BANK CO LTD\n",
      "CTBC BANK CO LTD CTBC BANK CO LTD\n",
      "CTBC BANK CO LTD CTBC BANK CO LTD\n",
      "CHANG HWA COMMERCIAL BANK LTD CHANG HWA COMMERCIAL BANK LTD\n",
      "STANDARD CHARTERED BANK STANDARD CHARTERED BANK\n",
      "STANDARD CHARTERED BANK STANDARD CHARTERED BANK\n",
      "STANDARD CHARTERED BANK STANDARD CHARTERED BANK\n",
      "AXIS BANK LIMITED AXIS BANK LIMITED\n",
      "AXIS BANK LIMITED AXIS BANK LIMITED\n",
      "AXIS BANK LIMITED AXIS BANK LIMITED\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    print(df['string_X_train'].values[i][df['string_Y_1'].values[i]:df['string_Y_2'].values[i]],df['Y_label'].values[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model and Tokenizer Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForQuestionAnswering: ['vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_transform.weight', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "258"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import DistilBertTokenizerFast\n",
    "from transformers import DistilBertForQuestionAnswering\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert-base-uncased\")\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocessing(df):\n",
    "    contexts = df['string_X_train'].values.tolist()\n",
    "    questions = [ 'What is the bank name?' for i in range(len(df))]\n",
    "    answers = []\n",
    "    for idx in df.index:\n",
    "        answers.append({\n",
    "            'text':df.loc[idx,'Y_label'],\n",
    "            'answer_start':df.loc[idx,'string_Y_1'],\n",
    "            'answer_end':df.loc[idx,'string_Y_2'],\n",
    "            })\n",
    "    return contexts ,questions ,answers\n",
    "\n",
    "train_contexts ,train_questions ,train_answers = preprocessing(train_df)\n",
    "val_contexts ,val_questions ,val_answers = preprocessing(val_df)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize our context/question pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_encodings = tokenizer(train_contexts, train_questions, truncation=True, padding=True)\n",
    "val_encodings = tokenizer(val_contexts, val_questions, truncation=True, padding=True)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add_token_positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def add_token_positions(encodings, answers):\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "    for i in range(len(answers)):\n",
    "        start_positions.append(encodings.char_to_token(i, answers[i]['answer_start']))\n",
    "        end_positions.append(encodings.char_to_token(i, answers[i]['answer_end'] - 1))\n",
    "\n",
    "        # if start position is None, the answer passage has been truncated\n",
    "        if start_positions[-1] is None:\n",
    "            start_positions[-1] = tokenizer.model_max_length\n",
    "        if end_positions[-1] is None:\n",
    "            end_positions[-1] = tokenizer.model_max_length\n",
    "\n",
    "    encodings.update({'start_positions': start_positions, 'end_positions': end_positions})\n",
    "\n",
    "add_token_positions(train_encodings, train_answers)\n",
    "add_token_positions(val_encodings, val_answers)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data is ready put it in a PyTorch dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "class SquadDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)\n",
    "\n",
    "train_dataset = SquadDataset(train_encodings)\n",
    "val_dataset = SquadDataset(val_encodings)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(device)\n",
    "model.to(device)\n",
    "model.train()\n",
    "train_loader = DataLoader(train_dataset, batch_size = 32, shuffle=True ,num_workers=4)\n",
    "val_loader = DataLoader(val_dataset, batch_size = 32, shuffle=True ,num_workers=4)\n",
    "optimizer = MADGRAD(model.parameters(),lr=5e-5)\n",
    "gc.collect()\n",
    "\n",
    "def train_step(model,batch,optimizer):\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    # forward\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    loss = outputs[0]\n",
    "    \n",
    "    # update model\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    gc.collect()\n",
    "    return loss.item()\n",
    "\n",
    "def val_step(model,batch,optimizer):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    # forward\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    start_positions = batch['start_positions'].to(device)\n",
    "    end_positions = batch['end_positions'].to(device)\n",
    "    outputs = model(input_ids, attention_mask=attention_mask, start_positions=start_positions, end_positions=end_positions)\n",
    "    loss = outputs[0]\n",
    "    \n",
    "    gc.collect()\n",
    "    return loss.item()\n",
    "\n",
    "def train_loop(model,train_loader,val_loader,optimizer,max_epochs=5,patience=3):\n",
    "    history = {'train_loss':[],'val_loss':[]}\n",
    "    best_loss = np.inf\n",
    "    best_model = None\n",
    "    not_improve_count = 0\n",
    "    for epoch in tqdm(range(max_epochs)):    \n",
    "        # reset this epoch loss equal to zero\n",
    "        epoch_train_loss = 0.0\n",
    "        epoch_val_loss = 0.0\n",
    "\n",
    "        # train one epoch and get train_loss\n",
    "        for i,batch in enumerate(tqdm(train_loader)):\n",
    "            epoch_train_loss += train_step(model,batch,optimizer)\n",
    "\n",
    "        # val one epoch and get val_loss\n",
    "        for j,batch in enumerate(tqdm(val_loader)):\n",
    "            epoch_val_loss += val_step(model,batch,optimizer)\n",
    "\n",
    "        # record loss history\n",
    "        history['train_loss'].append(epoch_train_loss/i)\n",
    "        history['val_loss'].append(epoch_val_loss/j)\n",
    "\n",
    "        # print this epoch's infomation\n",
    "        print(f'epoch:{epoch} train_loss:{epoch_train_loss/i} val_loss:{epoch_val_loss/j}')\n",
    "\n",
    "        # save best_model (if current val_loss <= best_loss)\n",
    "        if history['val_loss'][-1] <= best_loss: \n",
    "            best_model = deepcopy(model.eval())\n",
    "            best_loss = history['val_loss'][-1]\n",
    "            print(f'save best_model now_val_best_loss is:{best_loss}')\n",
    "\n",
    "        if history['val_loss'][-1] > best_loss:\n",
    "            not_improve_count += 1\n",
    "            print(f'not_improve_count:{not_improve_count}')\n",
    "            if not_improve_count > patience:\n",
    "                print('early_stoping')\n",
    "                break\n",
    "\n",
    "    # GET best_model.eval()\n",
    "    model = best_model.eval()\n",
    "    return model,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "25882b31be4044479d9c20536c2b15a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5b01552f54ae4392bcedf9d4e8daafbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "684ca79a75f547e18fd884c760b5add7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch:0 train_loss:2.204913732835225 val_loss:0.25849853083491325\n",
      "save best_model now_val_best_loss is:0.25849853083491325\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eafcc3699f084bb6a72ec091f53989c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c9225fd64ef4d4da5296f653242b06b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch:1 train_loss:0.2582014696938651 val_loss:0.16687751235440373\n",
      "save best_model now_val_best_loss is:0.16687751235440373\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "438a2cffadde4dd2bf33c60013ee78eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd8d02e5efae464a8650475cd4971cc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch:2 train_loss:0.18207298102123395 val_loss:0.2298128344118595\n",
      "not_improve_count:1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "410e34801ae141dab586ef37f189921f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa329b38a7594315b9d58fdc797e1b80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch:3 train_loss:0.1346452064279999 val_loss:0.13336914451792836\n",
      "save best_model now_val_best_loss is:0.13336914451792836\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8255c32b695542f380d78d12b5758407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48d8028eb2f4442c987b73bfa28eb65c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "epoch:4 train_loss:0.09849136216299874 val_loss:0.2120451219379902\n",
      "not_improve_count:2\n"
     ]
    }
   ],
   "source": [
    "model,history = train_loop(model,train_loader,val_loader,optimizer,max_epochs=5,patience=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loss curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history['train_loss'],label='train')\n",
    "plt.plot(history['val_loss'],label='val')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# question-answering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "nlp = pipeline('question-answering', model=model.to('cpu'), tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(df):\n",
    "    table = pd.DataFrame()\n",
    "    for i in tqdm(df.index):\n",
    "        sample = df.loc[[i]]\n",
    "        string_X_train = sample['string_X_train'].values[0]\n",
    "        string_Y_1 = sample['string_Y_1'].values[0]\n",
    "        string_Y_2 = sample['string_Y_2'].values[0]\n",
    "        QA_input = {\n",
    "            'question': 'What is the product name?',\n",
    "            'context': string_X_train\n",
    "        }\n",
    "        res = nlp(QA_input)\n",
    "        predict = QA_input['context'][res['start']:res['end']]\n",
    "        row = pd.DataFrame({\n",
    "            'label':string_X_train[string_Y_1:string_Y_2],\n",
    "            'predict:':predict},index=[i])\n",
    "        if string_X_train[string_Y_1:string_Y_2] == predict:\n",
    "            row['æ˜¯å¦å…¨å°'] = 'Yes'\n",
    "        else:\n",
    "            row['æ˜¯å¦å…¨å°'] = 'No'\n",
    "        table = table.append(row)\n",
    "        i += 1\n",
    "    return table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æœ€å¾Œè®“æˆ‘å€‘ä¾†çœ‹çœ‹æ¨¡åž‹åœ¨é©—è­‰é›†ä¸Šçš„è¡¨ç¾"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f041f4f1425648a68efeabf6b1074bba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>predict:</th>\n",
       "      <th>æ˜¯å¦å…¨å°</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2240</th>\n",
       "      <td>AGRICULTURAL BANK OF CHINA</td>\n",
       "      <td>AGRICULTURAL BANK OF CHINA</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>BDO UNIBANK INC</td>\n",
       "      <td>BDO UNIBANK INC</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1872</th>\n",
       "      <td>MIZUHO BANK LTD</td>\n",
       "      <td>MIZUHO BANK LTDx000D</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2483</th>\n",
       "      <td>CTBC BANK CO LTD</td>\n",
       "      <td>BANKERx000DCTBC BANK CO LTD</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>CTBC BANK CO LTD</td>\n",
       "      <td>CTBC BANK CO LTD</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3063</th>\n",
       "      <td>BANK OF CHINA LTD</td>\n",
       "      <td>ADDRESSBANK OF CHINA LTDXIAMEN</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2967</th>\n",
       "      <td>AGRICULTURAL BANK OF CHINA</td>\n",
       "      <td>THEx000DAGRICULTURAL BANK OF CHINA LTD</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2132</th>\n",
       "      <td>MUFG BANK LTD</td>\n",
       "      <td>MUFG BANK LTD</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2661</th>\n",
       "      <td>TAISHIN INTERNATIONAL BANK</td>\n",
       "      <td>TAISHIN INTERNATIONAL BANKx000D</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2713</th>\n",
       "      <td>BANK OF CHINA LTD</td>\n",
       "      <td>BANK OF CHINA LTD</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>99 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           label                                predict: æ˜¯å¦å…¨å°\n",
       "2240  AGRICULTURAL BANK OF CHINA              AGRICULTURAL BANK OF CHINA  Yes\n",
       "2639             BDO UNIBANK INC                         BDO UNIBANK INC  Yes\n",
       "1872             MIZUHO BANK LTD                    MIZUHO BANK LTDx000D   No\n",
       "2483            CTBC BANK CO LTD             BANKERx000DCTBC BANK CO LTD   No\n",
       "80              CTBC BANK CO LTD                        CTBC BANK CO LTD  Yes\n",
       "...                          ...                                     ...  ...\n",
       "3063           BANK OF CHINA LTD          ADDRESSBANK OF CHINA LTDXIAMEN   No\n",
       "2967  AGRICULTURAL BANK OF CHINA  THEx000DAGRICULTURAL BANK OF CHINA LTD   No\n",
       "2132               MUFG BANK LTD                           MUFG BANK LTD  Yes\n",
       "2661  TAISHIN INTERNATIONAL BANK         TAISHIN INTERNATIONAL BANKx000D   No\n",
       "2713           BANK OF CHINA LTD                       BANK OF CHINA LTD  Yes\n",
       "\n",
       "[99 rows x 3 columns]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_res = test_model(val_df.sample(99))\n",
    "all_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# claculate acc and jaccard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "No     63\n",
       "Yes    36\n",
       "Name: æ˜¯å¦å…¨å°, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.36363636363636365\n",
      "jaccard_avg_score: 0.6571341116795661\n"
     ]
    }
   ],
   "source": [
    "def get_jaccard_sim(str1, str2): \n",
    "    a = set(str1.split()) \n",
    "    b = set(str2.split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "display(all_res['æ˜¯å¦å…¨å°'].value_counts())\n",
    "acc = all_res['æ˜¯å¦å…¨å°'].value_counts()['Yes']/len(all_res)\n",
    "print('acc:',acc)\n",
    "jaccard_avg_score = np.mean([ get_jaccard_sim(all_res.label.loc[i],all_res['predict:'].loc[i]) for i in all_res.index])\n",
    "print('jaccard_avg_score:',jaccard_avg_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ”¾æ°´acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_acc(df,t=0.75):\n",
    "    correct = []\n",
    "    correct_label = []\n",
    "    for i in df.index:\n",
    "        jac = get_jaccard_sim(df.loc[i,'label'],df.loc[i,'predict:'])\n",
    "        if jac >= t:\n",
    "            correct.append('yes')\n",
    "        else:\n",
    "            correct.append('no')\n",
    "    result = pd.Series(correct)\n",
    "    return result.value_counts()['yes']/len(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40404040404040403"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_acc(all_res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2ccee5cc862501c1aad1cf19888c2a2eca4026b72d187db43d5e889cbdc6ee97"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
